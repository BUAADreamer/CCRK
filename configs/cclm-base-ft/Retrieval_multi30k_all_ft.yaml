# All-Language Finetune
train_file:  ['data/finetune/multi30k/train.en.json',
              'data/finetune/multi30k/train.de.json',
              'data/finetune/multi30k/train.fr.json',
              'data/finetune/multi30k/train.cs.json']

val_file: {'en': 'data/finetune/multi30k/val.en.json',
           'de': 'data/finetune/multi30k/val.de.json',
           'fr': 'data/finetune/multi30k/val.fr.json',
           'cs': 'data/finetune/multi30k/val.cs.json'}

test_file: {'en': 'data/finetune/multi30k/test_2016_flickr.en.json',
           'de': 'data/finetune/multi30k/test_2016_flickr.de.json',
           'fr': 'data/finetune/multi30k/test_2016_flickr.fr.json',
           'cs': 'data/finetune/multi30k/test_2016_flickr.cs.json'}

image_root: 'images/'

## Vision Encoder
vision_config: 'configs/config_swinB_384.json'

use_clip_vit: False
#image_res: 384
#patch_size: 16

use_swin: True
image_res: 384
patch_size: 32

## Text Encoder (& Cross Encoder)
text_encoder: 'data/xlm-roberta-large'
text_num_hidden_layers: 12



## Training
use_one_cl_proj_only: False

batch_size_train: 20
batch_size_test: 12
batch_size_test_text: 64
max_tokens: 40
embed_dim: 256
temp: 0.07
k_test: 128


## Other Settings
optimizer: {opt: adamW, lr: 3e-5, weight_decay: 0.01, lr_mult: 2}
schedular: {sched: linear, lr: 3e-5, epochs: 10, num_warmup_steps: 0.1}
